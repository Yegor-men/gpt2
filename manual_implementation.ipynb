{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Datamaker:\n",
    "    def __init__(self, vocabulary: str) -> None:\n",
    "        unique_vocabulary = list(set(vocabulary))\n",
    "        self.vocabulary_dict = {}\n",
    "        self.reverse_vocabulary_dict = {}\n",
    "        for index, letter in enumerate(unique_vocabulary):\n",
    "            self.vocabulary_dict[letter] = index\n",
    "            self.reverse_vocabulary_dict[index] = letter\n",
    "\n",
    "    def untokenize(self, x: torch.Tensor) -> list:\n",
    "        temp = []\n",
    "        for index, element in enumerate(x):\n",
    "            temp.append(self.reverse_vocabulary_dict[round(element.item())])\n",
    "        string = \"\".join(temp)\n",
    "        return string\n",
    "\n",
    "    def split_data(self, string_for_training: str, text_length: int) -> tuple:\n",
    "        split_string = list(string_for_training)\n",
    "        in_sequences, out_sequences = [], []\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i + text_length + 1 <= len(split_string):\n",
    "                temp_in = split_string[i:i + text_length]\n",
    "                temp_out = split_string[i + 1:i + text_length + 1]\n",
    "                for index, element in enumerate(temp_in):\n",
    "                    temp_in[index] = self.vocabulary_dict[element]\n",
    "                for index, element in enumerate(temp_out):\n",
    "                    temp_out[index] = self.vocabulary_dict[element]\n",
    "                in_sequences.append(temp_in)\n",
    "                out_sequences.append(temp_out)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        in_sequence_tensor = torch.zeros(len(in_sequences), text_length, dtype=torch.int)\n",
    "        out_sequence_tensor = torch.zeros(len(out_sequences), text_length, dtype=torch.long)\n",
    "\n",
    "        for sequence_index, sequence in enumerate(in_sequences):\n",
    "            for element_index, element in enumerate(sequence):\n",
    "                in_sequence_tensor[sequence_index, element_index] = element\n",
    "                out_sequence_tensor[sequence_index, element_index] = element\n",
    "\n",
    "        return in_sequence_tensor, out_sequence_tensor"
   ],
   "id": "eb6715d62de4d455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encode_size: int, vocab_size: int, context_length: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, encode_size)\n",
    "        self.positional_encodings = nn.Parameter(torch.randn(context_length, encode_size))\n",
    "        self.layer_norm = nn.LayerNorm(encode_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" x is of size [num_batches, sequence_length] \"\"\"\n",
    "        base_embedding = self.embedding(x)\n",
    "        pos_enc = self.positional_encodings\n",
    "        full_embedding = base_embedding + pos_enc\n",
    "        normalized = self.layer_norm(full_embedding)\n",
    "        return normalized\n"
   ],
   "id": "aa4a45c91b04f7a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, encode_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(encode_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.encode_size = encode_size\n",
    "        self.hidden_encode_size = encode_size // num_heads\n",
    "\n",
    "        self.q = nn.Linear(in_features=self.encode_size, out_features=self.encode_size)\n",
    "        self.k = nn.Linear(in_features=self.encode_size, out_features=self.encode_size)\n",
    "        self.v = nn.Linear(in_features=self.encode_size, out_features=self.encode_size)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=self.encode_size, out_features=self.encode_size * 4),\n",
    "            self.lrelu,\n",
    "            nn.Linear(in_features=self.encode_size * 4, out_features=self.encode_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "\n",
    "        num_batches = x.size(dim=0)\n",
    "        num_tokens = x.size(dim=1)\n",
    "        encode_size = x.size(dim=2)\n",
    "\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        \"\"\" q k v are size [batch_size, num_tokens, encode_size] \"\"\"\n",
    "\n",
    "        qs = torch.chunk(q, self.num_heads, dim=-1)\n",
    "        ks = torch.chunk(k, self.num_heads, dim=-1)\n",
    "        vs = torch.chunk(v, self.num_heads, dim=-1)\n",
    "        \"\"\" qs ks vs are num_heads tuples of size [batch_size, num_tokens, hidden_encode_size] \"\"\"\n",
    "\n",
    "        attention_matrices = torch.zeros(self.num_heads, num_batches, num_tokens, num_tokens)\n",
    "\n",
    "        for head in range(self.num_heads):\n",
    "            for batch in range(num_batches):\n",
    "                for q_index, query in enumerate(qs[head][batch]):\n",
    "                    for k_index, key in enumerate(ks[head][batch]):\n",
    "                        if k_index > q_index:\n",
    "                            attention_value = -torch.inf\n",
    "                        else:\n",
    "                            attention_value = torch.dot(query, key)\n",
    "                        attention_matrices[head][batch][q_index][k_index] = attention_value\n",
    "\n",
    "        attention_matrices = nn.Softmax(dim=-1)(attention_matrices)\n",
    "\n",
    "        head_outputs = []\n",
    "        for head in range(self.num_heads):\n",
    "            head_output = torch.bmm(attention_matrices[head], vs[head])\n",
    "            head_outputs.append(head_output)\n",
    "\n",
    "        values = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "        x = residual + values\n",
    "        x = self.layer_norm(x)\n",
    "        residual = x\n",
    "        mlp = self.mlp(x)\n",
    "        x = residual + mlp\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ],
   "id": "f4826ff1d0c98155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encode_size: int, vocab_amount: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=encode_size, out_features=encode_size * 4),\n",
    "            self.lrelu,\n",
    "            nn.Linear(in_features=encode_size * 4, out_features=vocab_amount)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "id": "99993af8e406330d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_heads: int, encode_size: int, vocab_size: int, context_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(encode_size=encode_size, vocab_size=vocab_size, context_length=context_size)\n",
    "        self.att_block_1 = AttentionBlock(num_heads=num_heads, encode_size=encode_size)\n",
    "        # self.att_block_2 = AttentionBlock(num_heads=num_heads, encode_size=encode_size)\n",
    "        # self.att_block_3 = AttentionBlock(num_heads=num_heads, encode_size=encode_size)\n",
    "        self.decoder = Decoder(encode_size=encode_size, vocab_amount=vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.att_block_1(x)\n",
    "        # x = self.att_block_2(x)\n",
    "        # x = self.att_block_3(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "id": "ec0827bfafc8cb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datamaker = Datamaker(\"abcdefghijklmnopqrstuvwxyz .\")\n",
    "in_data, out_data = datamaker.split_data(\"the quick brown fox jumps over the lazy dog.\", 8)\n",
    "print(f\"in: {in_data.size()}, out: {out_data.size()}\")\n",
    "\n",
    "model = Model(num_heads=8, encode_size=24, vocab_size=len(datamaker.vocabulary_dict),\n",
    "              context_size=8)"
   ],
   "id": "657045b392cb4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "flattened_out_data = torch.flatten(out_data, start_dim=0, end_dim=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"E {epoch + 1}/{EPOCHS} - {((epoch + 1) / EPOCHS) * 100:.2f}%\")\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    raw_logits = model(in_data)\n",
    "    flattened_logits = torch.flatten(raw_logits, start_dim=0, end_dim=1)\n",
    "    sample_data = flattened_logits[:8]\n",
    "    sample_argmax = torch.argmax(sample_data, dim=-1)\n",
    "    human_readable_sample_data = datamaker.untokenize(sample_argmax)\n",
    "    print(f\"Sample: {human_readable_sample_data}\")\n",
    "\n",
    "    loss = loss_fn(flattened_logits, flattened_out_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "id": "111feb71cecdd0d0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
