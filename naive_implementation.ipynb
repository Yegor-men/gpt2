{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T12:59:01.418598Z",
     "start_time": "2025-02-12T12:59:01.416422Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:05:57.635054Z",
     "start_time": "2025-02-12T15:05:57.630963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encode_size: int, letters: str) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.letters = letters\n",
    "        letters_list = list(letters)\n",
    "        self.letters_dict = {}\n",
    "        for index, letter in enumerate(letters_list):\n",
    "            self.letters_dict[letter] = index\n",
    "\n",
    "        self.vocabulary = nn.Parameter(torch.randn(encode_size, len(self.letters_dict)))\n",
    "\n",
    "    def tokenize(self, string: str) -> torch.Tensor:\n",
    "        indices = [self.letters_dict[letter] for letter in string]\n",
    "        tensor_text = torch.zeros(len(self.letters_dict), len(indices))\n",
    "        for index, letter in enumerate(indices):\n",
    "            tensor_text[letter, index] = 1.0\n",
    "        print(tensor_text.size())\n",
    "        return tensor_text\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        print(self.vocabulary.size(), x.size())\n",
    "        embedded_tokens = torch.matmul(self.vocabulary, x)\n",
    "        # TODO positional embedding\n",
    "        # TODO normalize the stuff\n",
    "        return embedded_tokens"
   ],
   "id": "aa4a45c91b04f7a6",
   "outputs": [],
   "execution_count": 243
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, encode_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.encode_size = encode_size\n",
    "        self.hidden_encode_size = encode_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(in_features=self.encode_size, out_features=self.hidden_encode_size)\n",
    "        self.key = nn.Linear(in_features=self.encode_size, out_features=self.hidden_encode_size)\n",
    "        self.value = nn.Linear(in_features=self.encode_size, out_features=self.hidden_encode_size)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=self.encode_size, out_features=self.hidden_encode_size * 4),\n",
    "            self.lrelu,\n",
    "            nn.Linear(in_features=self.hidden_encode_size * 4, out_features=self.hidden_encode_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        queries, keys, values = self.query(x), self.key(x), self.value(x)\n",
    "\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "\n",
    "        # TODO figure out the rest of the optimization mumbo jumbo\n",
    "\n",
    "        return x"
   ],
   "id": "f4826ff1d0c98155"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encode_size: int, vocab_amount: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=encode_size, out_features=encode_size * 4),\n",
    "            self.lrelu,\n",
    "            nn.Linear(in_features=encode_size * 4, out_features=vocab_amount)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "id": "99993af8e406330d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_heads: int, encode_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(encode_size=encode_size, letters=\"abcdefghijklmnopqrstuvwxyz .,\")\n",
    "        self.attention_block_1 = AttentionBlock(num_heads=num_heads, encode_size=encode_size)\n",
    "        self.attention_block_2 = AttentionBlock(num_heads=num_heads, encode_size=encode_size)\n",
    "        self.attention_block_3 = AttentionBlock(num_heads=num_heads, encode_size=encode_size)\n",
    "        self.decoder = Decoder(encode_size=encode_size, vocab_amount=len(self.encoder.letters_dict))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.attention_block_1(x)\n",
    "        x = self.attention_block_2(x)\n",
    "        x = self.attention_block_3(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "id": "ec0827bfafc8cb82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
